# Reproducability of experiments
To reproduce the experiments follow this guide.
In this ZIP file you will find our experiment results and instructions in the `results` directory.
In the following we will describe how to reproduce these results yourself.

The current BETZE system can be found in the `betze` directory.
After acceptance of this paper, it will be published on GitHub. 

## Requirements
The following guide assumes a Linux system with Docker and Python installed.

## Data acquisition
The NoBench datasets can be generated by downloading the generator from [here](http://www.cs.wisc.edu/quickstep/argo/nobench.tar.bz2).
For the sake of simplicity we assume all datasets will be located in the `/data` directory.
If you want to store it somewhere else, replace this path with your desired path.
To generate our datasets you then have to call:

``` bash
# 5.5 Mb dataset
mkdir -p /data/nobench/10000
./nobench_gendata.py -m -n 10000 -o /data/10000/NoBench.json
rm /data/10000/NoBench_extra.json

# 55 Mb dataset
mkdir -p /data/nobench/100000
./nobench_gendata.py -m -n 100000 -o /data/nobench/100000/NoBench.json
rm /data/nobench/100000/NoBench_extra.json

# 550 Mb dataset
mkdir -p /data/nobench/1000000
./nobench_gendata.py -m -n 1000000 -o /data/nobench/1000000/NoBench.json
rm /data/nobench/1000000/NoBench_extra.json

# 5.5 Gb dataset
mkdir -p /data/nobench/10000000
./nobench_gendata.py -m -n 10000000 -o /data/nobench/10000000/NoBench.json
rm /data/nobench/10000000/NoBench_extra.json

# 30 Gb dataset
mkdir -p /data/nobench/50000000
./nobench_gendata.py -m -n 50000000 -o /data/nobench/50000000/NoBench.json
rm /data/nobench/50000000/NoBench_extra.json
```

For the Twitter data, please refer to the official [Twitter manual](https://developer.twitter.com/en/docs/twitter-api/tweets/volume-streams/introduction) on how to fetch a sampled dataset.
We assume that this file has been stored in the `/data/twitter/Twitter.json` file in line-separated
JSON format.
The generated sessions from a Twitter dataset acquired through the IP will probably differ from our generated queries.
But hosting the actual 109Gb dataset we used is unfeasible and legally unclear. 
If it is desired, to better reproduce the results please contact nschaefer@cs.uni-kl.de. 

The Reddit dataset can be retrieved from [https://files.pushshift.io/reddit/comments/](https://files.pushshift.io/reddit/comments/).




## Query Generation & Benchmarking
To generate benchmark queries, you have to go into the `betze` directory and call the `generate_queries.sh <dataset-dir> <query-dir> [<seed>]` script.

We once again assume that the queries will be located in the `/queries` directory, split into directories for every benchmark.

Executing a query session is done by calling the `./benchmark_queries.sh <dataset-dir> <query-dir>`.
We will store the results in their own `/results` directory. 

Our `/data` directory has been mounted on a ramdisk, to make the disk-based systems a little bit more competitive.

In the following we will list the commands used to create and benchmark the sessions for the experiments in our paper, in order of appearance in the paper.

### User Configuration trend

To generate the queries for the user configuration trends, we used the following commands:

```bash
for seed in {1..30}; do
    for user in "novice" "intermediate" "expert"; do
        echo "Generating sessions for '$user' with seed $seed"
        mkdir -p "/queries/user_trends/$user/$seed"
        ./generate_queries.sh /data/twitter /queries/user_trends/$user/$seed $seed --preset $user --num_queries 20
      done
done
```

This will iterate over all user presets and generate 20 queries for 30 different seeds.
To benchmark these queries we then used:

```bash
for seed in {1..30}; do
    for user in "novice" "intermediate" "expert"; do
      echo "Executing session for '$user' with seed $seed"
      RESULT_DIR="/results/user_trends/$user/$seed"
      mkdir -p "$RESULT_DIR"
      cp "/results/user_trends/$user/$seed/queries.joda" "$RESULT_DIR/queries.joda"
      ./benchmark_queries.sh "/data/twitter" "$RESULT_DIR" &> "$RESULT_DIR/run.log"
  done
done
```

### User Preset execution times

For the execution times we used the same commands, just without the `--num_queries 20` argument:

```bash
for seed in {1..30}; do
    for user in "novice" "intermediate" "expert"; do
        echo "Generating sessions for '$user' with seed $seed"
        mkdir -p "/queries/user_execution/$user/$seed"
        ./generate_queries.sh /data/twitter /queries/user_execution/$user/$seed $seed --preset $user
      done
done
```

This will iterate over all user presets and generate 20 queries for 30 different seeds.
To benchmark these queries we then used:

```bash
for seed in {1..30}; do
    for user in "novice" "intermediate" "expert"; do
      echo "Executing session for '$user' with seed $seed"
      RESULT_DIR="/results/user_execution/$user/$seed"
      mkdir -p "$RESULT_DIR"
      cp "/results/user_execution/$user/$seed/queries.joda" "$RESULT_DIR/queries.joda"
      ./benchmark_queries.sh "/data/twitter" "$RESULT_DIR" &> "$RESULT_DIR/run.log"
  done
done
```

### Jump configuration evaluation

To generate queries for all jump probability configurations we used the following commands:


```bash
for seed in {1..20}; do
  for j in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9; do
    for g in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9; do
      # If j + g >= 1 then skip, use awk
      if [ "$(awk "BEGIN{print $j + $g >= 1}")" -ge 1 ]; then
          continue
      fi
      echo "Generating sessions for jump = $j, goback = $g with seed $seed"
      QUERY_DIR="/queries/probabilities/$j+$g/$seed" 
      mkdir -p "$QUERY_DIR"
      ./generate_queries.sh /data/twitter $QUERY_DIR $seed --probability-randomjump "$j" --probability-backtrack "$g" --num_queries 10
    done
  done
done
```

To execute these queries we then used:

```bash
for seed in {1..20}; do
  for j in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9; do
    for g in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9; do
        if [ "$(awk "BEGIN{print $j + $g >= 1}")" -ge 1 ]; then
          continue
        fi
        echo "Executing session for jump = $j, goback = $g with seed $seed"
        RESULT_DIR="/results/probabilities/$j+$g/$seed"
        mkdir -p "$RESULT_DIR"
        cp "/queries/probabilities/$j+$g/$seed/queries.joda" "$RESULT_DIR/queries.joda"
        ./benchmark_queries.sh "/data/twitter" "$RESULT_DIR" &> "$RESULT_DIR/run.log"
    done
  done
done
```

### CPU scalability

We only used a single session with seed `123` on the Twitter dataset to evaluate the CPU scalability of the systems:

```bash
./generate_queries.sh /data/twitter /queries/threads 123
```

To execute this session with different number of threads we used:

```bash
for run in {1..3}; do
  for thread in 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60; do
    RESULT_DIR="/results/threads/$thread/$run"
    mkdir -p "$RESULT_DIR"
    cp "/queries/threads/"* "$RESULT_DIR/".
    echo "=== $thread Threads ==="
    ./benchmark_queries.sh "/data/twitter" "$RESULT_DIR" "--cpuset-cpus=1-$thread" &> "$RESULT_DIR/run.log"
  done
done
```

### Dataset scalability

Similarly, we also only used one session to evaluate the dataset similarity.
Here we used only used one of the NoBench datasets to generate the session, as we wanted to execute the same queries on all sizes.
If we created the sessions for all datasets, the results would probably not be comparable.

```bash
./generate_queries.sh /data/nobench/10000000 /queries/dataset 123
```

To execute the benchmark, we then used:

```bash
for repeat in {1..20}; do
  echo "= $repeat Run ="
  for ds in 10000 100000 1000000 10000000 50000000; do
  echo 
    RESULT_DIR="/results/dataset/$ds/$repeat"
    mkdir -p "$RESULT_DIR"
    cp "$QUERY_DIR/"* "$RESULT_DIR/".
    echo "= $ds Dataset ="
    ./benchmark_queries.sh "/data/nobench/$ds" "$RESULT_DIR" &> "$RESULT_DIR/run.log"
  done
done
```

### Session execution time
As noted in the evaluation chapter, the results of these benchmarks have been taken from the previous two.

### System comparison

To compare all systems we used another custom script to generate and evaluate all benchmark queries.
Additionally, in contrast to all other evaluations we imported the datasets once and evaluated all queries for the given system.
If we would have imported the dataset for each benchmark again, the evaluation would have taken multiple months to complete.

```bash 
datasets=("Twitter" "Reddit" "NoBench")
users=("novice" "intermediate" "expert")
configs=("" "--aggregate --exclude-aggregation GroupBy" "--aggregate" )
seeds=(1 2 3)

for seed in "${seeds[@]}"; do
  for dataset in "${datasets[@]}"; do
    # Start JODA server
    dataset_dir=/datasets/"$dataset"
    docker run --pull always --rm -d  -v "$dataset_dir:/dataset" -p "5632:5632" --name explorer-joda ghcr.io/joda-explore/joda/joda:0.13.1 || exit 1

    # Import dataset in server
    docker exec explorer-joda joda-client --address localhost --port 5632 --query "LOAD $dataset FROM FILE \"/dataset/$dataset.json\" LINESEPARATED" -c 0 || exit 1

    #Generate queries
    for config_i in "${!configs[@]}"; do
      config="${configs[$config_i]}"
        for user in "${users[@]}"; do
            echo "Generating sessions for dataset $dataset, config '$config' with '$user' and seed '$seed'"
            subdir="$dataset"_"$user"_"$config_i"_"$seed"
            mkdir -p "$QUERY_DIR"/"$subdir"
            q_dir=/data/$subdir
            docker run --rm --add-host=host.docker.internal:host-gateway -v "$QUERY_DIR:/data" random-explorer generate --seed "$seed" --preset "$user" --betze-file "$q_dir"/betze.json --joda-file "$q_dir"/query.joda --mongo-file "$q_dir"/mongo.js --joda-file "$q_dir"/queries.joda --jq-file "$q_dir"/jq.sh --psql-file "$q_dir"/postgres.sql $config  --joda-host http://host.docker.internal:5632 /data/"$dataset".json || exit 1
        done
      done
    
    # Stop server
    docker stop explorer-joda
  done
done
```

To execute these generated queries we used the following custom script:

```bash
DS_DIR="/datasets" 
datasets=("Twitter" "Reddit" "NoBench")
users=("novice" "intermediate" "expert")
configs=("" "--aggregate --exclude-aggregation GroupBy" "--aggregate" )
seeds=(1 2 3)

for repeat in {1..3}; do
  echo "--------- Run $repeat ---------"
  for dataset in "${datasets[@]}"; do
    echo "----- Dataset: $dataset -----"
    dataset_dir=$DS_DIR/$dataset
    # PSQL
    psql_cont=$(docker run --name=psql-benchmark -e POSTGRES_PASSWORD=postgres -e POSTGRES_HOST_AUTH_METHOD=trust --pull always -d  --tmpfs /var/lib/postgresql/data -v "$dataset_dir:/dataset"  "$PSQL_IMAGE"  -c log_statement=all -c log_duration=on)
    sleep 10
    # - Table
    echo "CREATE UNLOGGED TABLE $dataset (doc jsonb);" > init.sql
    # - Import
    echo "COPY $dataset (doc) from program 'sed -e ''s/\\\\/\\\\\\\\/g'' /dataset/$dataset.json';"  >> init.sql
    # - Execute
    echo "--- Importing PSQL Dataset ---"
    docker exec psql-benchmark mkdir /data
    docker cp init.sql psql-benchmark:/data/init.sql || exit 1
    rm init.sql
    docker exec psql-benchmark psql -U postgres -f /data/init.sql 

    # MongoDB
    mongo_cont=$(docker run --name=mongo-benchmark --pull always -d --tmpfs /data -v "$dataset_dir:/dataset" "$MONGO_IMAGE")
    echo "--- Importing MongoDB Dataset ---"
    docker exec mongo-benchmark mongoimport --db benchmark --collection $dataset --file /dataset/$dataset.json || exit 1
    docker exec mongo-benchmark mkdir /queries
    
    for seed in "${seeds[@]}"; do
      for config_i in "${!configs[@]}"; do
          for user in "${users[@]}"; do
              echo "Benchmarking sessions for dataset $dataset, config '$config_i' with '$user' and seed '$seed'"
              subdir="$QUERY_DIR"/"$dataset"_"$user"_"$config_i"_"$seed"
              resultdir="$subdir"/results/"$repeat"
              mkdir -p "$resultdir"
              cp "$subdir/"*.* "$resultdir"
              # Change JODA file if every query should be outputted to dev null
              if [ "$config_i" -eq 0 ]; then
              sed -e 's/$/STORE AS FILE "\/dev\/null"/' -i "$resultdir"/queries.joda
              fi
              # Benchmark given config, with dataset and a timeout of 2 hours
              # The script was modified to not import the datasets again
              ./benchmark_queries.sh "$DS_DIR/$dataset" "$resultdir" "7200"  > "$resultdir"/run.log || exit 1
          done
        done
    done
    # Cleanup containers
    docker stop "$mongo_cont" && docker rm "$mongo_cont"
    docker stop "$psql_cont" && docker rm "$psql_cont"
  done
done
```

It imports the dataset for MongoDB and PostgreSQL once before the queries, and executes all queries fo the given dataset.
The `benchmark_queries.sh` script was modified to not import the datasets again.